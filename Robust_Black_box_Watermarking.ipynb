{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Robust Black-box Watermarking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPBIsV0595hojSogseKT18G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilzeevi25/Black-box-Watermarking-tfidf/blob/main/Robust_Black_box_Watermarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Black-box Watermarking for Deep Neural Network using Inverse Document Frequency\n",
        "Based on Yadollahi et al. https://arxiv.org/pdf/2103.05590\n",
        "## Implemented on PAN12 DataSet: \n",
        "https://pan.webis.de/clef12/pan12-web/sexual-predator-identification.html\n"
      ],
      "metadata": {
        "id": "faLDzaQr9Nyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j9NTtTxfaD5",
        "outputId": "f310976f-e98e-4414-8e2b-9556584eb25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKiDt1yRDkF4",
        "outputId": "50bda63a-4a51-456b-d6ef-c44b70935dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/STTM\n"
          ]
        }
      ],
      "source": [
        "cd /content/gdrive/MyDrive/STTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecadfa8c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import datetime\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "import csv\n",
        "import sklearn\n",
        "import string\n",
        "import pickle\n",
        "import random as python_random\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "from keras import backend as K \n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "QUGvwFXdR0rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_dict(data_path):\n",
        "    labels_dict = {}\n",
        "    with open(data_path + 'sci_labels.csv', 'r') as f:\n",
        "        file = csv.reader(f)\n",
        "        for row in file:\n",
        "            labels_dict[row[0]] = row[1]\n",
        "    return labels_dict\n",
        "\n",
        "\n",
        "def get_features_labels(root, labels_dict):\n",
        "    corpus = [] # each row is a string formed from all messages in a conversations\n",
        "    labels = [] # each row is 0 or 1, corresponds to label for same row in corpus\n",
        "\n",
        "    for conversation in root:\n",
        "        string = \" \"\n",
        "        for message in conversation:\n",
        "            text = message.find('text').text\n",
        "            if text is not None:\n",
        "                #preprocess:\n",
        "                # text = preprocess_text(text)\n",
        "                string = string + \"\\r\\n\" + text \n",
        "        corpus.append(string)\n",
        "        labels.append(int(labels_dict[conversation.get('id')]))\n",
        "    return corpus, labels"
      ],
      "metadata": {
        "id": "XVXznllfRvYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"data/pan12-sexual-predator-identification-training-corpus-2012-05-01/\"\n",
        "\n",
        "training_xml = ET.parse(train_data_path + 'training_data.xml')\n",
        "train_root = training_xml.getroot()\n",
        "\n",
        "test_data_path = 'data/pan12-sexual-predator-identification-test-corpus-2012-05-21/'\n",
        "test_xml = ET.parse(test_data_path + 'pan12-sexual-predator-identification-test-corpus-2012-05-17.xml')\n",
        "test_root = test_xml.getroot()\n",
        "\n",
        "train_corpus, train_labels = get_features_labels(train_root, get_labels_dict(train_data_path))\n",
        "test_corpus, test_labels = get_features_labels(test_root, get_labels_dict(test_data_path))"
      ],
      "metadata": {
        "id": "8jsUAsdBR444"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a DNN"
      ],
      "metadata": {
        "id": "dMgVMd6F-Snb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer()\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# PAN12 data\n",
        "X_train = vectorizer.fit_transform(train_corpus).astype('float16')\n",
        "X_test = vectorizer.transform(test_corpus).astype('float16')\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ],
      "metadata": {
        "id": "544cG2md-dvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training \n",
        "print (\"Create model ... \")\n",
        "def build_model(shape):\n",
        "    # K.clear_session()\n",
        "    np.random.seed(12)\n",
        "    python_random.seed(12)\n",
        "    tf.random.set_seed(12)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=shape, activation='relu'))\n",
        "    # model.add(Dense(256, input_dim=121394, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(200, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(160, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(120, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(80, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "print(\"Compile model ...\")\n",
        "modeldnn = build_model(X_train.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa_y9pZX-GRH",
        "outputId": "2c1d4b41-b545-4da2-efed-76e779c05072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create model ... \n",
            "Compile model ...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               31077120  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               51400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 160)               32160     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 160)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 120)               19320     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 80)                9680      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 80)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 81        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,189,761\n",
            "Trainable params: 31,189,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "modeldnn.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "modeldnn.fit(X_train, y_train, batch_size=64, shuffle=\"batch\",\n",
        "                epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqKfrLmG-kCP",
        "outputId": "5689fd9a-7a29-4882-bcae-8beb885994dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 4s 12ms/step - loss: 0.0985 - accuracy: 0.9587\n",
            "Epoch 2/3\n",
            "230/230 [==============================] - 3s 12ms/step - loss: 0.0396 - accuracy: 0.9973\n",
            "Epoch 3/3\n",
            "230/230 [==============================] - 3s 12ms/step - loss: 0.0096 - accuracy: 0.9982\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe0dffc5090>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = modeldnn.predict(X_test)\n",
        "pred_y =np.round(pred_y.flatten())\n",
        "print('Accuracy on PAN12 test dataset: ',metrics.accuracy_score(y_test, pred_y)*100)\n",
        "print('F1_Score on PAN12 test dataset: ',metrics.f1_score(y_test, pred_y)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GswWBpz8-pEB",
        "outputId": "57322dff-2ea4-492c-dea6-2383fe53eab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on PAN12 test dataset:  98.04161724511371\n",
            "F1_Score on PAN12 test dataset:  65.39863325740319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, After training a Network, we can apply our watermark"
      ],
      "metadata": {
        "id": "3x9lzElM_Qii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly select *B* samples for each class from training set. To create a fair and balanced trigger set,\n",
        "the number of samples selected from each class is equal."
      ],
      "metadata": {
        "id": "Lemm09-2jRib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will choose **B = 50** conversations for each class"
      ],
      "metadata": {
        "id": "i8on38oBjxRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 50"
      ],
      "metadata": {
        "id": "MNLK-OgLouy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(12)\n",
        "labels_df = pd.Series(train_labels)\n",
        "benign_samps_idx = labels_df[labels_df == 0].sample(B).index.tolist() #indices of 10 benign conversations\n",
        "mal_samps_idx = labels_df[labels_df == 1].sample(B).index.tolist() #indices of 10 malicious content conversations"
      ],
      "metadata": {
        "id": "WnRsCGmzjUvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the TF-IDF score for each word in all documents. <br>\n",
        "we uniform the words by changing them to lowercase, and\n",
        "removing the punctuation and stop words"
      ],
      "metadata": {
        "id": "MQ5wB2KFlAtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_df = pd.Series(train_corpus)\n",
        "corpus_df = corpus_df.apply(lambda x: \"\".join([ch for ch in x if ch not in string.punctuation]))"
      ],
      "metadata": {
        "id": "KKdvMlygjMB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english',lowercase=True)\n",
        "corpus_tfidf = vectorizer.fit_transform(corpus_df.tolist())\n",
        "vocab_map = {y: x for x, y in vectorizer.vocabulary_.items()}"
      ],
      "metadata": {
        "id": "kv_ZyH2Dlz5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the following steps For each selected document from a given class:\n",
        "* Randomly select one document from another class to exchange their words and\n",
        "producing a watermark record.\n",
        "* Select K words of both documents with lowest TF-IDF score.\n",
        "* Exchange the selected words and swap the labels of two documents.\n",
        "* Insert the modified documents into the trigger set."
      ],
      "metadata": {
        "id": "1j0tzFrQrUpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_K_words(tfidf_mat,mapping,K):\n",
        "  tmp = pd.Series(tfidf_mat.toarray()[0])\n",
        "  tmp.index =tmp.index.map(mapping)\n",
        "  return tmp[tmp > 0].nsmallest(K).index.tolist()"
      ],
      "metadata": {
        "id": "IPtC-58bvvtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 16\n",
        "trigger_set= [] #Assign the trigger set\n",
        "trigger_labels = []\n",
        "origin_idx = []\n",
        "while benign_samps_idx:\n",
        "  random.shuffle(benign_samps_idx)\n",
        "  random.shuffle(mal_samps_idx)\n",
        "  doc_1_idx = benign_samps_idx.pop() #doc1 represents benign content\n",
        "  doc_2_idx = mal_samps_idx.pop() #doc1 represents malicious content\n",
        "  origin_idx.extend([doc_1_idx,doc_2_idx])\n",
        "  words_doc_1 = get_K_words(corpus_tfidf[doc_1_idx],vocab_map,K)\n",
        "  words_doc_2 = get_K_words(corpus_tfidf[doc_2_idx],vocab_map,K)\n",
        "  swapped_1 = pd.Series(corpus_df[doc_1_idx]).replace({a:b for a,b in zip(words_doc_1, words_doc_2)},regex=True).tolist()[0]\n",
        "  swapped_2 = pd.Series(corpus_df[doc_2_idx]).replace({b:a for a,b in zip(words_doc_1, words_doc_2)},regex=True).tolist()[0]\n",
        "  trigger_set.extend([swapped_1,swapped_2])\n",
        "  trigger_labels.extend([1,0])"
      ],
      "metadata": {
        "id": "rGJuDTeVqKbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trigger_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ4ZsDr-5uXu",
        "outputId": "a2cafebc-e003-4eda-b592-396a0a812abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-train the model and see if the accuracy is affected"
      ],
      "metadata": {
        "id": "BpL9lZBa5ry9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Re-train the model and see if the accuracy is affected"
      ],
      "metadata": {
        "id": "y6cU_aAfvhrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus.extend(trigger_set)\n",
        "train_labels.extend(trigger_labels)"
      ],
      "metadata": {
        "id": "O8Xgwlmn390g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer()\n",
        "vectorizer_train = TfidfVectorizer()\n",
        "\n",
        "# PAN12 data\n",
        "X_train = vectorizer_train.fit_transform(train_corpus).astype('float16')\n",
        "X_test = vectorizer_train.transform(test_corpus).astype('float16')\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ],
      "metadata": {
        "id": "EsUCxfQgAEv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeldnn = build_model(X_train.shape[1])\n",
        "# Train model\n",
        "modeldnn.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "modeldnn.fit(X_train, y_train, batch_size=64, shuffle=\"batch\",\n",
        "                epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjacX2oyAPfW",
        "outputId": "6d5195d6-d5c7-4d8c-9ce1-24d46339f876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 256)               31295488  \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 200)               51400     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 160)               32160     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 160)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 120)               19320     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 80)                9680      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 80)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 81        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,408,129\n",
            "Trainable params: 31,408,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_1/dense_6/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_1/dense_6/embedding_lookup_sparse/Reshape:0\", shape=(None, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_1/dense_6/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232/232 [==============================] - 4s 12ms/step - loss: 0.1203 - accuracy: 0.9570\n",
            "Epoch 2/3\n",
            "232/232 [==============================] - 3s 12ms/step - loss: 0.0364 - accuracy: 0.9905\n",
            "Epoch 3/3\n",
            "232/232 [==============================] - 3s 12ms/step - loss: 0.0210 - accuracy: 0.9936\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1e0f04750>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = modeldnn.predict(X_test)\n",
        "pred_y =np.round(pred_y.flatten())\n",
        "print('Accuracy on PAN12 test dataset: ',metrics.accuracy_score(y_test, pred_y)*100) # previous acc: 98.04\n",
        "print('F1_Score on PAN12 test dataset: ',metrics.f1_score(y_test, pred_y)*100) # previous f1: 65.39"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4mE00R3ALSp",
        "outputId": "e3cf204e-cf93-4136-8b63-3fc6d496863d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on PAN12 test dataset:  98.00809653963178\n",
            "F1_Score on PAN12 test dataset:  63.414634146341456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the triggered set which was created with 100 conversations in total, and inserted into the training data caused test-set accuracy to deteriorate from `98.04%` into `98%`which is almost as the model was not affected by adding trigger set at all, whereas the the F1 was damaged from `65.39` to `63.41`"
      ],
      "metadata": {
        "id": "D8MqZT2l6Pv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets see if the trigger set and the original set yields different classifications:"
      ],
      "metadata": {
        "id": "mAsicc0E-rrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus, train_labels = get_features_labels(train_root, get_labels_dict(train_data_path))\n",
        "corpus_df = pd.Series(train_corpus)"
      ],
      "metadata": {
        "id": "a9dJSgGkCBaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tr = vectorizer_train.transform(trigger_set)\n",
        "pred_y_tr = modeldnn.predict(X_test_tr.astype('float16'))\n",
        "pred_y_tr =np.round(pred_y_tr.flatten())\n",
        "\n",
        "X_test_org = vectorizer_train.transform(corpus_df.loc[origin_idx].tolist())\n",
        "pred_y_org = modeldnn.predict(X_test_org.astype('float16'))\n",
        "pred_y_org =np.round(pred_y_org.flatten())\n",
        "\n",
        "print(f'Successful {str(100-int(metrics.accuracy_score(pred_y_org, pred_y_tr)*100))} watermarked conversations on trigger set out of {B*2} possible')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EpXdoYH_3zz",
        "outputId": "6a8cfa09-35ea-4d39-c59e-4a8169e865d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful 60 watermarked conversations on trigger set out of 100 possible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the paper states, we may take θ  instances (θ is athreshold) as a subset of the trigger set, to determine if our model was compromised."
      ],
      "metadata": {
        "id": "DGZihwPbaSQE"
      }
    }
  ]
}